# Gpt Invention

Start from here!

---

<!-- .slide: data-background="#999999" -->

<!-- .slide: data-background-iframe="media/flow.html" -->

---

## Data

---

### Annual Report (10-k filings)

- åŸä½œï¼š
    - [Financial Modeling Prep](https://site.financialmodelingprep.com/developer/docs/#Annual-Reports-on-Form-10-K)
    - è²»ç”¨ï¼š29 ç¾å…ƒ/æœˆ
- å»ºè­°ï¼š
    - ä½¿ç”¨çˆ¬èŸ² [sec-edgar](https://github.com/sec-edgar/sec-edgar) åœ¨ [SEC ç¾åœ‹è­‰åˆ¸äº¤æ˜“å§”å“¡æœƒ](https://www.sec.gov/edgar/searchedgar/companysearch) æŠ“å– 10-k filings
    - å…è²»
    - å…è²»

----

### Stock Price
- total 1505 stocks
    - S&P500, S&P400, S&P600
- åŸä½œï¼š
    - [`openbb`](https://openbb.co/)
- å»ºè­°ï¼š
    - [`yfinance`](https://finance.yahoo.com/)ï¼šYahoo! Finance

----

<!-- .slide: data-background="https://hackmd.io/_uploads/BktMtfN1a.png" -->


---

### Sample

- train: 1k datapoints (out of 17.4k)
- test: 500 datapoints (out of 6.8k) 



## Cost: 

- Money: $60
- Time: 50 hours


---

## Embedding Model


### all-mpnet-base-v2
This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.
https://huggingface.co/sentence-transformers/all-mpnet-base-v2


```python!
embedding_model = LangchainEmbedding(   HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")
    ) 
```

---

### Massive Text Embedding Benchmark
https://huggingface.co/blog/mteb
![](https://hackmd.io/_uploads/Bkq8xozJT.png)

---

![](https://hackmd.io/_uploads/B1NrxsG16.png)

---

ğŸ Maximum speed Models like Glove offer high speed, but suffer from a lack of context awareness resulting in low average MTEB scores.

âš–ï¸ Speed and performance Slightly slower, but significantly stronger, ==all-mpnet-base-v2== or all-MiniLM-L6-v2 provide a good balance between speed and performance.

ğŸ’ª Maximum performance Multi-billion parameter models like ST5-XXL, GTR-XXL or SGPT-5.8B-msmarco dominate on MTEB. They tend to also produce bigger embeddings like SGPT-5.8B-msmarco which produces 4096 dimensional embeddings requiring more storage!

---

## Vector DB

Chroma

---

## Questions

27 questions

```json
{
    "management":"Does the company have a clear strategy for growth and innovation? Are there any recent strategic initiatives or partnerships?"
}

---

```
1. feature_overall	
2. feature_revenue_1	
3. feature_revenue_2	
4. feature_revenue_3	
5. feature_profit_1	
6. feature_profit_2	
7. feature_debt_1	
8. feature_cashflow_2	
9. feature_dividend	
10. feature_management_1	
11. feature_management_2	
12. feature_industry_1	
13. feature_industry_2	
14. feature_research	
15. feature_guidance	
16. feature_leadership	
17. feature_macro	
18. feature_diversification	
19. feature_customerbase	
20. feature_esg	
21. feature_competition_1	
22. feature_competition_2	
23. feature_ip	
24. feature_digitaltransformation
25. feature_regulations	
26. feature_onlinepresence	
27. feature_legal


---

## Model

- Linear Regression: enforces non-negativity in the coefficients


- X
    - Confidence Scores from GPT-3.5-Turbo
- Y: percentage return is calculated between two successive annual report filing dates
    - target_custom_22 
        - Annual Return
        - target created from 12 month normalised returns per year (aka era) and then binned
    - target_custom_2
        - 98th percentile of return from the filing date represented as target max
        - target created from max normalised returns in the span of 12 months per year (aka era) and then binned


---

## Result

----


<!-- .slide: data-background="https://hackmd.io/_uploads/BksjiDtr6.png" -->

----

<!-- .slide: data-background="https://hackmd.io/_uploads/HyC4hwYST.png" -->


----


<!-- .slide: data-background="https://hackmd.io/_uploads/B1CVnPKHp.png" -->

----

<!-- .slide: data-background="https://hackmd.io/_uploads/HyCVnwFra.png" -->

----




<!-- .slide: data-background="https://hackmd.io/_uploads/rJCE2PFHa.png" -->

----

<!-- .slide: data-background="https://hackmd.io/_uploads/rJ0VhvFB6.png" -->

----

- æŒ‘é¸è‚¡ç¥¨
    - æ¯å€‹æœˆæœˆåº•é æ¸¬ä¸‹å€‹æœˆæœ€å¼·çš„äº”æª”è‚¡ç¥¨
    - è‹¥ä¸Šå€‹æœˆæ‰€æŒ‘å‡ºä¾†çš„è‚¡ç¥¨æ¯”è¼ƒå¼·ï¼Œå‰‡æœƒè¢«ä¿ç•™ä¸‹ä¾†
    - ç¶œåˆä¸Šè¿°åšæ³•ï¼ŒæŒ‘å‡ºäº”æª”è‚¡ç¥¨
- æœˆåˆé–‹ç›¤é€²å ´ï¼Œæœˆåº•æ”¶ç›¤å‡ºå ´


----

<!-- .slide: data-background-iframe="media/cumprod_amount_2018_2022.html" -->


----

<!-- .slide: data-background-iframe="media/cumprod_amount_2002_2022.html" -->


----

- é¿é–‹çªç™¼çš„æå¤±
    - ä¸‹è·Œ 10 % å°±åœæ­¢äº¤æ˜“
    - è§€å¯Ÿä¸‹å€‹æœˆçš„æƒ…æ³ï¼Œè‹¥å¯ç²åˆ©ï¼Œå°±é€²å ´ä¸‹å–®


----

<!-- .slide: data-background-iframe="media/cumprod_amount_2018_2022_trick.html" -->


----


<!-- .slide: data-background-iframe="media/cumprod_amount_2002_2022_trick.html" -->

----

<!-- .slide: data-background-iframe="media/annual_return_2002_2022.html" -->


----

- å¹´åŒ–å ±é…¬ç‡ï¼š14%
- æœ€å¤§äº¤æ˜“å›è½ï¼š28%

----

<!-- .slide: data-background-iframe="media/cumprod_amount_2023.html" -->

---

åˆ†å¹´åˆ†é…ç›®æ¨™å€¼ï¼š
ç›®çš„ï¼šç‚ºäº†åœ¨æ¯å¹´å…§ç›¸å°æ’åè‚¡ç¥¨çš„å›å ±ã€‚
æ–¹æ³•ï¼šæ¯å¹´å–®ç¨åˆ†é…è‚¡ç¥¨çš„ç›®æ¨™å€¼ï¼Œé€™æ¨£å¯ä»¥æ›´å¥½åœ°æ¯”è¼ƒåŒä¸€å¹´å…§ä¸åŒè‚¡ç¥¨çš„è¡¨ç¾ã€‚

å›å ±çš„æ’åå’Œæ¨™æº–åŒ–ï¼š
é¦–å…ˆï¼šå›å ±é¦–å…ˆè¢«æ’åï¼Œé€™æ„å‘³è‘—æ ¹æ“šå…¶å›å ±ç‡å°è‚¡ç¥¨é€²è¡Œæ’åºã€‚
ç„¶å¾Œï¼šç„¶å¾Œå°æ’åå¾Œçš„å›å ±é€²è¡Œæ¨™æº–åŒ–ï¼Œé€™æœ‰åŠ©æ–¼æ¶ˆé™¤æ•¸æ“šä¸­çš„ä»»ä½•æ½›åœ¨åå·®æˆ–ç•°å¸¸å€¼ã€‚

ç›®æ¨™å€¼çš„ç¯„åœé™åˆ¶ï¼š
ç¯„åœï¼š[0, 1]ã€‚
è§£é‡‹ï¼šå…¶ä¸­1è¡¨ç¤ºæ›´é«˜çš„å›å ±ï¼Œé€™æ„å‘³è‘—ç›®æ¨™å€¼1è¡¨ç¤ºè©²è‚¡ç¥¨åœ¨è©²å¹´åº¦æœ‰æ›´é«˜çš„å›å ±ã€‚

åŸºæ–¼ç™¾åˆ†ä½æ•¸çš„åˆ†ç®±ï¼š
æœ€å¾Œï¼šå°æ¨™æº–åŒ–çš„å›å ±é€²è¡Œåˆ†ç®±ï¼ŒåŸºæ–¼ç™¾åˆ†ä½æ•¸ï¼Œé€™æ¨£ç›®æ¨™å€¼çš„ç¯„åœæ˜¯[0, 1]ã€‚
ç›®çš„ï¼šé€™æœ‰åŠ©æ–¼å‰µå»ºä¸€å€‹æ›´å‡å‹»å’Œæœ‰åºçš„ç›®æ¨™å€¼åˆ†å¸ƒï¼Œå¯ä»¥æ›´å¥½åœ°ç”¨æ–¼æ©Ÿå™¨å­¸ç¿’æ¨¡å‹çš„è¨“ç·´ã€‚

----

